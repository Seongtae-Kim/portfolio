{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "Experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-30T08:41:13.628450Z",
          "start_time": "2021-01-30T08:40:57.355647Z"
        },
        "scrolled": true,
        "id": "WKbAPmBoIp5y",
        "outputId": "fabaefa0-7ae2-4719-a915-a60eb52af2d1"
      },
      "source": [
        "from benchmark import Lexical_Benchmark\n",
        "be = Lexical_Benchmark()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================================\n",
            "UWIN: Total definitions in dictionary: 182,371\n",
            "UWIN: WordMap Built\n",
            "40 unknown word ids cleared\n",
            "117 invalid hypernyms are deleted\n",
            "104635 invalid hyponyms are deleted\n",
            "UWIN built\n",
            "\n",
            "URIMAL_SAM: Total words in dictionary: 1,139,969\n",
            "URIMAL_SAM: Total senses in dictionary: 422,677\n",
            "URIMAL_SAM built\n",
            "\n",
            "==================================================================\n",
            "CORPUS: MODU built: 7,265 sentences\n",
            "CORPUS: Sejong built: 2,355 sentences\n",
            "CORPUS: modu+sejong concatenated: 9,620 sentences\n",
            "==================================================================\n",
            "WORD_DICTIONARY: 6,669 unique lexemes and 9,393 unique scodes are assembled (sejong, modu)\n",
            "==================================================================\n",
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "VECTORIZERS: general vectorizers created\n",
            "==================================================================\n",
            "VECTORS: General Vectors Built.\n",
            "VECTORS: Alternative Vectors Built.\n",
            "==================================================================\n",
            "==================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KhLcUrzIp5z"
      },
      "source": [
        "import os, re, pickle\n",
        "\n",
        "classifiers={}\n",
        "path = \"./classifiers/\"\n",
        "for name in os.listdir(path):\n",
        "    model = re.search(\"(bert|w2v|ft)\", name).group()\n",
        "    lex = re.search(\"[가-힣]+\", name).group()\n",
        "    mode = re.search(\"(general|alterative)\", name).group()\n",
        "    if model not in classifiers:\n",
        "        classifiers.update({model: {lex: pickle.load(open(path+name, \"rb\"))}})\n",
        "    else:\n",
        "        classifiers[model].update({lex: pickle.load(open(path+name, \"rb\"))})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qzzTLFPIp50"
      },
      "source": [
        "import pickle\n",
        "w2v_general_vecs = pickle.load(\n",
        "    open(\"./vectors/concated_w2v_general_vectors.pkl\", \"rb\"))\n",
        "ft_general_vecs = pickle.load(\n",
        "    open(\"./vectors/concated_ft_general_vectors.pkl\", \"rb\"))\n",
        "bert_general_vecs = pickle.load(\n",
        "    open(\"./vectors/concated_bert_general_vectors.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8YmyVBEIp50"
      },
      "source": [
        "def get_general_vector(vectors):\n",
        "        vector = vectors[0][0]  # first vector\n",
        "        labels = []\n",
        "        for i, (v, l) in enumerate(vectors):\n",
        "            if i != 0:\n",
        "                vector = np.concatenate((vector, v), axis=0)\n",
        "            labels.append(l)\n",
        "        vectors = vector.copy()\n",
        "        return vectors, labels\n",
        "\n",
        "# train uniformed sum vector\n",
        "def get_uniform_vector(vectors):\n",
        "    vector = vectors[0]  # first vector\n",
        "    labels = []\n",
        "    for i, v in enumerate(vectors):\n",
        "        if i != 0:\n",
        "            vector += v\n",
        "    vectors = vector\n",
        "    return vectors\n",
        "\n",
        "# train weighted sum vector\n",
        "def get_weighted_vector(vectors, frequencies):\n",
        "    whole_freq = sum([freq for sclass, freq in frequencies[lexeme].items()])\n",
        "\n",
        "    target_s_freq = frequencies[vectors[0][1]][vectors[0][2]]\n",
        "    a = target_s_freq/whole_freq\n",
        "    vector = a*vectors[0][0]\n",
        "    labels = []\n",
        "    for i, (v, lex, s, l) in enumerate(vectors):\n",
        "        if i != 0:\n",
        "            try:\n",
        "                target_s_freq = frequencies[lex][s]\n",
        "            except KeyError:\n",
        "                print(s, lex)\n",
        "                raise\n",
        "            a = target_s_freq/whole_freq\n",
        "            vector += (a*v)\n",
        "        labels.append(l)\n",
        "    vectors = vector\n",
        "    return vectors, labels\n",
        "\n",
        "# get random indices within certain percent\n",
        "def random_indices(length, exclusion=[], percentage=0.3):\n",
        "    import random\n",
        "\n",
        "    l = list(range(length))\n",
        "    result = []\n",
        "    chance = int(len(l)*percentage)\n",
        "\n",
        "    while chance > 0:\n",
        "        chosen = random.choice(l)\n",
        "        if chosen not in exclusion:\n",
        "            chance-=1\n",
        "            result.append(chosen)\n",
        "            index = l.index(chosen)\n",
        "            l.remove(chosen)\n",
        "    return result\n",
        "\n",
        "def return_random_values_of_list(l, exclusion=[], percentage=0.3):\n",
        "    indicies = random_indices(len(l), exclusion, percentage)\n",
        "\n",
        "    new_list=[]\n",
        "    for index in indicies:\n",
        "        new_list.append(l[index])\n",
        "    assert len(new_list) == int(len(l)*percentage)\n",
        "    \n",
        "    return new_list, indicies\n",
        "\n",
        "\n",
        "def investigate_sclass_classifiers(vecs, model, mode=\"general\", percentage=0.5, quiet=False):\n",
        "    from sklearn import neural_network as nn\n",
        "    from tqdm.notebook import tqdm\n",
        "    from random import shuffle\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    import random\n",
        "    import os, re\n",
        "\n",
        "    sclasses=set()\n",
        "    for vec in vecs:\n",
        "        if vec is not None:\n",
        "            sclasses.add(vec[4]) # sclass name\n",
        "    sclasses.remove(None)\n",
        "    \n",
        "    results = [\"{} / {} 결과\\n\".format(model, mode)]\n",
        "    \n",
        "    with tqdm(sclasses, leave=False, bar_format=\"{percentage:2.2f}% {bar} {desc} | {remaining}\") as t:\n",
        "        for s, target_sclass in enumerate(sclasses):\n",
        "            mlp = nn.MLPClassifier(max_iter=300,\n",
        "                                    activation=\"relu\",\n",
        "                                    hidden_layer_sizes=(64, 64, 64),\n",
        "                                    solver=\"adam\")\n",
        "            \n",
        "            target_cnt=0\n",
        "            non_target_cnt=0\n",
        "\n",
        "            target_vecs_training=[]\n",
        "            non_target_vecs_training=[]\n",
        "            \n",
        "            target_vecs_test=[]\n",
        "            non_target_vecs_test=[]\n",
        "\n",
        "            for n, vec in enumerate(vecs):\n",
        "                t.set_description_str(\"센스 클래스: {}({}/{}) | 벡터 번호 {}/{}\".format(target_sclass, s+1, len(sclasses), n+1, len(vecs)))\n",
        "                if vec is not None:\n",
        "                    test = vec[0]\n",
        "                    training = vec[1]\n",
        "                    lexeme = vec[2]\n",
        "                    scode = vec[3]\n",
        "                    sclass = vec[4]\n",
        "\n",
        "                    if training is not None:\n",
        "                        if target_sclass == sclass: # positive\n",
        "                            target_vecs_training.append((training, 1))\n",
        "                            target_vecs_test.append((test, 1))\n",
        "                            target_cnt+=1\n",
        "                        else: # negative\n",
        "                            non_target_vecs_training.append((training, 0))\n",
        "                            non_target_vecs_test.append((test, 0))\n",
        "                            non_target_cnt+=1\n",
        "\n",
        "            # Splitting (test/train 50:50)\n",
        "\n",
        "            target_vecs_training, exclusion_target = return_random_values_of_list(target_vecs_training, percentage=percentage)\n",
        "            target_vecs_test, _ = return_random_values_of_list(target_vecs_test, exclusion_target, percentage)\n",
        "\n",
        "            non_target_vecs_training, exclusion_non_target = return_random_values_of_list(non_target_vecs_training, percentage=percentage)\n",
        "            non_target_vecs_test, _ = return_random_values_of_list(non_target_vecs_test, exclusion_non_target, percentage)\n",
        "\n",
        "            # Making test / train data with the same numbers\n",
        "            if target_cnt < non_target_cnt:\n",
        "                mn_val = target_cnt \n",
        "            else:\n",
        "                mn_val = non_target_cnt\n",
        "\n",
        "            target_vecs_training = target_vecs_training[:mn_val]\n",
        "            target_vecs_test = target_vecs_test[:mn_val]\n",
        "            non_target_vecs_training = non_target_vecs_training[:mn_val]\n",
        "            non_target_vecs_test = non_target_vecs_test[:mn_val]\n",
        "\n",
        "            training_vectors = target_vecs_training + non_target_vecs_training\n",
        "            vectors_test = target_vecs_test + non_target_vecs_test\n",
        "            \n",
        "            # Shuffling\n",
        "            shuffle(training_vectors)\n",
        "            shuffle(vectors_test)\n",
        "        \n",
        "            # TRAINING\n",
        "            vector = training_vectors[0][0]  # first vector\n",
        "            labels_training = []\n",
        "            for i, (v, l) in enumerate(training_vectors):\n",
        "                if i != 0:\n",
        "                    vector = np.concatenate((vector, v), axis=0)\n",
        "                labels_training.append(l)\n",
        "            training_vectors = vector\n",
        "            \n",
        "            mlp = mlp.fit(training_vectors, np.array(labels_training))\n",
        "\n",
        "\n",
        "            score = mlp.score(vectors_test, np.array(labels_test))\n",
        "\n",
        "\n",
        "            # Writing Results & Saving Classifier\n",
        "            result = \"센스 클래스 '{}'| positive: {:,} / negative: {:,} |정확도: {}\\n\".format(target_sclass, target_cnt, non_target_cnt, round(score, 2))\n",
        "            \n",
        "            if not quiet:\n",
        "                print(result, end=\"\")\n",
        "\n",
        "            results.append(result)\n",
        "            with open(\"./results/{}_{}_accuracies.txt\".format(model, mode), \"w\") as f:\n",
        "                f.write(\"\".join(results))\n",
        "            with open(\"./classifiers/{}_{}_{}.pkl\".format(target_sclass, model, mode), \"wb\") as f:\n",
        "                pickle.dump(mlp, f)\n",
        "\n",
        "            t.update()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYM8JAFlIp54"
      },
      "source": [
        "import pickle\n",
        "w2v_general_vecs = pickle.load(open(\"./vectors/concated_w2v_general_vectors.pkl\", \"rb\"))\n",
        "ft_general_vecs = pickle.load(open(\"./vectors/concated_ft_general_vectors.pkl\", \"rb\"))\n",
        "bert_general_vecs = pickle.load(open(\"./vectors/concated_bert_general_vectors.pkl\", \"rb\"))\n",
        "frequencies = pickle.load(open(\"./dictionary/matched_words/frequencies.pkl\", \"rb\"))\n",
        "sense_classes = pickle.load(open(\"./dictionary/sense_classes.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvwU5eFSIp55"
      },
      "source": [
        "def train_ambiguity_classifier(vecs, model, sense_classes, mode=\"general\", percentage=0.5, quiet=False):\n",
        "    from sklearn import neural_network as nn\n",
        "    from tqdm.notebook import tqdm\n",
        "    from random import shuffle\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    import random\n",
        "    import os, re\n",
        "\n",
        "    sclasses=set()\n",
        "    for vec in vecs:\n",
        "        if vec is not None:\n",
        "            sclasses.add(vec[4]) # sclass name\n",
        "    sclasses.remove(None)\n",
        "\n",
        "    mlp = nn.MLPClassifier(max_iter=300,\n",
        "                                activation=\"relu\",\n",
        "                                hidden_layer_sizes=(64, 64, 64),\n",
        "                                solver=\"adam\")\n",
        "        \n",
        "    \n",
        "    non_target_vecs_training=[]\n",
        "    non_target_vecs_test=[]\n",
        "    target_vecs_training=[]\n",
        "    target_vecs_test=[]\n",
        "    non_target_cnt=0\n",
        "    target_cnt=0\n",
        "\n",
        "    with tqdm(vecs, leave=False, bar_format=\"{percentage:2.2f}% {bar} {desc} | {remaining}\") as t:\n",
        "        for n, vec in enumerate(vecs):\n",
        "            t.update()\n",
        "            t.set_description_str(\"벡터 번호 {}/{}\".format(n+1, len(vecs)))\n",
        "            if vec is not None:\n",
        "                test = vec[0]\n",
        "                training = vec[1]\n",
        "                lexeme = vec[2]\n",
        "                scode = vec[3]\n",
        "                sclass = vec[4]\n",
        "\n",
        "                if training is not None:\n",
        "                    if len(sense_classes[lexeme]) >= 2: # ambiguous\n",
        "                        target_vecs_training.append((training, 1))\n",
        "                        #target_vecs_test.append((test, 1))\n",
        "                        target_cnt+=1\n",
        "                    else: # unambiguous\n",
        "                        non_target_vecs_training.append((training, 0))\n",
        "                        #non_target_vecs_test.append((test, 0))\n",
        "                        non_target_cnt+=1\n",
        "\n",
        "    training_vectors = target_vecs_training + non_target_vecs_training\n",
        "    \n",
        "    # Shuffling\n",
        "    shuffle(training_vectors)\n",
        "\n",
        "    # TRAINING\n",
        "    vector = training_vectors[0][0]  # first vector\n",
        "    labels_training = []\n",
        "    for i, (v, l) in enumerate(training_vectors):\n",
        "        if i != 0:\n",
        "            vector = np.concatenate((vector, v), axis=0)\n",
        "        labels_training.append(l)\n",
        "    training_vectors = vector\n",
        "    \n",
        "    mlp = mlp.fit(training_vectors, np.array(labels_training))\n",
        "\n",
        "    # Writing Results & Saving Classifier\n",
        "\n",
        "    with open(\"./classifiers/{}_{}_({}_{}).pkl\".format(model, \"ambiguity_detector.classifier\", target_cnt, non_target_cnt), \"wb\") as f:\n",
        "        pickle.dump(mlp, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ce1415f2828e4ceb8d876d9633d7f7a3",
            "ec417f5c16fc4589b979ce98677874cb",
            "6ac676104bc849f08cdb6848a15cdb46"
          ]
        },
        "id": "CRaMJx1jIp56",
        "outputId": "814010b6-c54f-48f4-ecaf-86f434dfe60f"
      },
      "source": [
        "train_ambiguity_classifier(w2v_general_vecs, model=\"w2v\", sense_classes=sense_classes)\n",
        "train_ambiguity_classifier(ft_general_vecs, model=\"ft\", sense_classes=sense_classes)\n",
        "train_ambiguity_classifier(bert_general_vecs, model=\"bert\", sense_classes=sense_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44115.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce1415f2828e4ceb8d876d9633d7f7a3"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44115.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec417f5c16fc4589b979ce98677874cb"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=45095.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ac676104bc849f08cdb6848a15cdb46"
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiFFlrbWIp57"
      },
      "source": [
        "def get_general_vector(vectors):\n",
        "        vector = vectors[0][0]  # first vector\n",
        "        for i, v in enumerate(vectors):\n",
        "            if i != 0:\n",
        "                vector = np.concatenate((vector, v), axis=0)\n",
        "        return vector\n",
        "\n",
        "# train uniformed sum vector\n",
        "def get_uniform_vector(vectors):\n",
        "    vector = vectors[0]  # first vector\n",
        "    labels = []\n",
        "    for i, v in enumerate(vectors):\n",
        "        if i != 0:\n",
        "            vector += v\n",
        "    vectors = vector\n",
        "    return vectors\n",
        "\n",
        "\n",
        "# train weighted sum vector\n",
        "def get_weighted_vector(vectors, frequencies):\n",
        "    whole_freq = sum([freq for scode, (sclass, freq) in frequencies[vectors[0][1]].items()])\n",
        "    try:\n",
        "        target_s_freq = frequencies[vectors[0][1]][vectors[0][2]][1]\n",
        "        a = target_s_freq/whole_freq\n",
        "    except KeyError:\n",
        "        print(vectors[0][1], vectors[0][2])\n",
        "    \n",
        "    vector = a*vectors[0][0]\n",
        "    labels = []\n",
        "    for i, (v, lex, s) in enumerate(vectors):\n",
        "        if i != 0:\n",
        "            try:\n",
        "                target_s_freq = frequencies[lex][s][1]\n",
        "            except KeyError:\n",
        "                print(s, lex)\n",
        "                raise\n",
        "            a = target_s_freq/whole_freq\n",
        "            vector += (a*v)\n",
        "    vectors = vector\n",
        "    return vectors\n",
        "\n",
        "def ambiguity_experiment(model, mode, vecs, sense_classes, frequencies):\n",
        "    assert mode in [\"general\", \"uniformed\", \"weighted\"]\n",
        "\n",
        "    # get all lexemes\n",
        "    lexs=set()\n",
        "    frequencies={}\n",
        "\n",
        "    for vec in vecs:\n",
        "        if vec is None:\n",
        "            continue\n",
        "        #test = vec[0]\n",
        "        #training = vec[1]\n",
        "        lexeme = vec[2]\n",
        "        scode = vec[3]\n",
        "        sclass = vec[4]\n",
        "        \n",
        "        lexs.add(lexeme)\n",
        "        if scode is not None:\n",
        "            if lexeme in frequencies:\n",
        "                if scode in frequencies[lexeme]:\n",
        "                    frequencies[lexeme][scode][1]+=1\n",
        "                else:\n",
        "                    frequencies[lexeme].update({scode:[sclass, 1]})\n",
        "\n",
        "            else:\n",
        "                frequencies.update({lexeme:{scode: [sclass, 1]}})\n",
        "\n",
        "    from statistics import mean\n",
        "    from tqdm import tqdm\n",
        "    from collections import Counter\n",
        "    import os, pickle\n",
        "\n",
        "    if model == \"w2v\":\n",
        "        path = \"./classifiers_all/ambiguity detection/w2v_ambiguity_detector.classifier_(2627_966).pkl\"\n",
        "    elif model == \"ft\":\n",
        "        path = \"./classifiers_all/ambiguity detection/ft_ambiguity_detector.classifier_(2627_966).pkl\"\n",
        "    elif model == \"bert\":\n",
        "        path = \"./classifiers_all/ambiguity detection/bert_ambiguity_detector.classifier_(2627_966).pkl\"\n",
        "\n",
        "    mlp = pickle.load(open(path, \"rb\"))\n",
        "\n",
        "    strings=[\"lexeme\\tambiguous\\tmode\\tmodel\\tscore\\n\"]\n",
        "    \n",
        "    scores=[]\n",
        "    answer_sheet=[]\n",
        "\n",
        "    for lex in tqdm(lexs):\n",
        "        target_vectors =[]\n",
        "        ambiguity_labels=[] # if 1, it is ambiguous\n",
        "\n",
        "        for vec in vecs:\n",
        "            if vec is not None:\n",
        "                test = vec[0]\n",
        "                training = vec[1]\n",
        "                lexeme = vec[2]\n",
        "                scode = vec[3]\n",
        "                sclass = vec[4]\n",
        "                if lexeme == lex and test is not None:\n",
        "                    if mode != \"weighted\":\n",
        "                        target_vectors.append(test)\n",
        "                    elif mode == \"weighted\":\n",
        "                        target_vectors.append((test, lexeme, scode))\n",
        "                    \n",
        "\n",
        "        if len(target_vectors)==0:\n",
        "            continue\n",
        "        \n",
        "        if mode == \"uniformed\":\n",
        "            vector = get_uniform_vector(target_vectors)\n",
        "        elif mode == \"weighted\":\n",
        "            vector = get_weighted_vector(target_vectors, frequencies)\n",
        "\n",
        "\n",
        "        answer = 1 if len(frequencies[lex])>=2 else 0 # ambiguity answer by lex\n",
        "        if mode in [\"uniformed\", \"weighted\"]:\n",
        "            binary = mlp.predict(vector)[0]\n",
        "            if binary == answer: # correct\n",
        "                answer_sheet.append(1)\n",
        "            else: # incorrect\n",
        "                answer_sheet.append(0)\n",
        "\n",
        "        else: # for general vectors\n",
        "            for vector in target_vectors:\n",
        "                binary = mlp.predict(vector)[0]\n",
        "\n",
        "                if binary == answer: # correct\n",
        "                    answer_sheet.append(1)\n",
        "                else: # incorrect\n",
        "                    answer_sheet.append(0)\n",
        "\n",
        "        \n",
        "        corrects = Counter(answer_sheet)[1]\n",
        "        score = round((corrects /len(answer_sheet))*100, 4)\n",
        "        scores.append(score)\n",
        "        strings.append(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(lex, answer , mode, model, score))    \n",
        "        with open(\"./results/ambiguity_{}_{}_result.tsv\".format(model, mode), \"w\") as f:\n",
        "            f.write(\"\".join(strings))\n",
        "\n",
        "    strings.append(\"\\nAverage score: {}\\n\".format(round(mean(scores), 4)))\n",
        "    with open(\"./results/ambiguity_{}_{}_result.tsv\".format(model, mode), \"w\") as f:\n",
        "        f.write(\"\".join(strings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZfUBhOiIp58"
      },
      "source": [
        "import pickle\n",
        "w2v_general_vecs = pickle.load(open(\"./vectors/concated_w2v_general_vectors.pkl\", \"rb\"))\n",
        "ft_general_vecs = pickle.load(open(\"./vectors/concated_ft_general_vectors.pkl\", \"rb\"))\n",
        "bert_general_vecs = pickle.load(open(\"./vectors/concated_bert_general_vectors.pkl\", \"rb\"))\n",
        "frequencies = pickle.load(open(\"./dictionary/matched_words/frequencies.pkl\", \"rb\"))\n",
        "sense_classes = pickle.load(open(\"./dictionary/sense_classes.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9oQbjUIp58",
        "outputId": "bd398b41-879c-48ee-e390-ea836ea5151b"
      },
      "source": [
        "ambiguity_experiment(\"w2v\", \"general\", w2v_general_vecs, sense_classes, frequencies)\n",
        "ambiguity_experiment(\"ft\", \"general\", ft_general_vecs, sense_classes, frequencies)\n",
        "ambiguity_experiment(\"bert\", \"general\", bert_general_vecs, sense_classes, frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6455/6455 [01:56<00:00, 55.47it/s]\n",
            "100%|██████████| 6455/6455 [02:11<00:00, 49.14it/s]\n",
            "100%|██████████| 6455/6455 [01:07<00:00, 95.05it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J2x1oZOIp58",
        "outputId": "0c7f0ad3-3a44-4e2d-b9c5-0b74ea20ed63"
      },
      "source": [
        "ambiguity_experiment(\"w2v\", \"uniformed\", w2v_general_vecs, sense_classes, frequencies)\n",
        "ambiguity_experiment(\"ft\", \"uniformed\", ft_general_vecs, sense_classes, frequencies)\n",
        "ambiguity_experiment(\"bert\", \"uniformed\", bert_general_vecs, sense_classes, frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6455/6455 [01:47<00:00, 60.03it/s]\n",
            "100%|██████████| 6455/6455 [02:01<00:00, 52.96it/s]\n",
            "100%|██████████| 6455/6455 [01:24<00:00, 76.75it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBlzyTgwIp59",
        "outputId": "1ba9f4fd-ec92-4ed6-fb1f-a2fcdc96d2c5"
      },
      "source": [
        "ambiguity_experiment(\"w2v\", \"weighted\", w2v_general_vecs, sense_classes, frequencies)\n",
        "ambiguity_experiment(\"ft\", \"weighted\", ft_general_vecs, sense_classes, frequencies)\n",
        "ambiguity_experiment(\"bert\", \"weighted\", bert_general_vecs, sense_classes, frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6455/6455 [01:57<00:00, 55.15it/s]\n",
            "100%|██████████| 6455/6455 [01:57<00:00, 54.78it/s]\n",
            "100%|██████████| 6455/6455 [01:14<00:00, 86.37it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "FDbJ3IJmIp59"
      },
      "source": [
        "def ambiguity_baseline(vecs):\n",
        "    # get all lexemes\n",
        "    lexs=set()\n",
        "    frequencies={}\n",
        "\n",
        "    for vec in vecs:\n",
        "        if vec is None:\n",
        "            continue\n",
        "        #test = vec[0]\n",
        "        #training = vec[1]\n",
        "        lexeme = vec[2]\n",
        "        scode = vec[3]\n",
        "        sclass = vec[4]\n",
        "        \n",
        "        lexs.add(lexeme)\n",
        "        if scode is not None:\n",
        "            if lexeme in frequencies:\n",
        "                if scode in frequencies[lexeme]:\n",
        "                    frequencies[lexeme][scode][1]+=1\n",
        "                else:\n",
        "                    frequencies[lexeme].update({scode:[sclass, 1]})\n",
        "\n",
        "            else:\n",
        "                frequencies.update({lexeme:{scode: [sclass, 1]}})\n",
        "\n",
        "    from collections import Counter\n",
        "    from statistics import mean\n",
        "    \n",
        "    strings=[\"lexeme\\tambiguous\\tscore\\n\"]\n",
        "    \n",
        "    scores=[]\n",
        "    answer_sheet=[]\n",
        "\n",
        "    for lex in lexs:\n",
        "        \n",
        "        answer = 1 if len(frequencies[lex])>=2 else 0 # ambiguity answer by lex\n",
        "        binary = int(random()*2) # random answer\n",
        "        if binary == answer: # correct\n",
        "            answer_sheet.append(1)\n",
        "        else: # incorrect\n",
        "            answer_sheet.append(0)\n",
        "        \n",
        "        corrects = Counter(answer_sheet)[1]\n",
        "        score = round((corrects /len(answer_sheet))*100, 4)\n",
        "        scores.append(score)\n",
        "        strings.append(\"{}\\t{}\\t{}\\n\".format(lex, answer, score))    \n",
        "        with open(\"./results/ambiguity_{}_result.tsv\".format(\"baseline\"), \"w\") as f:\n",
        "            f.write(\"\".join(strings))\n",
        "\n",
        "    strings.append(\"\\nAverage score: {}\\n\".format(round(mean(scores), 4)))\n",
        "    with open(\"./results/ambiguity_{}_result.tsv\".format(\"baseline\"), \"w\") as f:\n",
        "        f.write(\"\".join(strings))\n",
        "\n",
        "import pickle\n",
        "w2v_general_vecs = pickle.load(open(\"./vectors/concated_w2v_general_vectors.pkl\", \"rb\"))\n",
        "ambiguity_baseline(w2v_general_vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CCeAVYRIp5-"
      },
      "source": [
        "def get_general_vector(vectors):\n",
        "        vector = vectors[0][0]  # first vector\n",
        "        for i, v in enumerate(vectors):\n",
        "            if i != 0:\n",
        "                vector = np.concatenate((vector, v), axis=0)\n",
        "        return vector\n",
        "\n",
        "# train uniformed sum vector\n",
        "def get_uniform_vector(vectors):\n",
        "    vector = vectors[0]  # first vector\n",
        "    labels = []\n",
        "    for i, v in enumerate(vectors):\n",
        "        if i != 0:\n",
        "            vector += v\n",
        "    vectors = vector\n",
        "    return vectors\n",
        "\n",
        "\n",
        "# train weighted sum vector\n",
        "def get_weighted_vector(vectors, frequencies):\n",
        "    whole_freq = sum([freq for scode, (sclass, freq) in frequencies[vectors[0][1]].items()])\n",
        "    try:\n",
        "        target_s_freq = frequencies[vectors[0][1]][vectors[0][2]][1]\n",
        "        a = target_s_freq/whole_freq\n",
        "    except KeyError:\n",
        "        print(vectors[0][1], vectors[0][2])\n",
        "    \n",
        "    vector = a*vectors[0][0]\n",
        "    labels = []\n",
        "    for i, (v, lex, s) in enumerate(vectors):\n",
        "        if i != 0:\n",
        "            try:\n",
        "                target_s_freq = frequencies[lex][s][1]\n",
        "            except KeyError:\n",
        "                print(s, lex)\n",
        "                raise\n",
        "            a = target_s_freq/whole_freq\n",
        "            vector += (a*v)\n",
        "    vectors = vector\n",
        "    return vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OvvO-WaIp5-"
      },
      "source": [
        "def experiment(model, mode, vecs, frequencies):\n",
        "    assert mode in [\"general\", \"uniformed\", \"weighted\"]\n",
        "\n",
        "    # get all lexemes\n",
        "    lexs=set()\n",
        "    sclasses=set()\n",
        "    frequencies={}\n",
        "\n",
        "    for vec in vecs:\n",
        "        if vec is None:\n",
        "            continue\n",
        "        #test = vec[0]\n",
        "        #training = vec[1]\n",
        "        lexeme = vec[2]\n",
        "        scode = vec[3]\n",
        "        sclass = vec[4]\n",
        "        \n",
        "        lexs.add(lexeme)\n",
        "        sclasses.add(sclass)\n",
        "        if sclass is not None:\n",
        "            if lexeme in frequencies:\n",
        "                if scode in frequencies[lexeme]:\n",
        "                    frequencies[lexeme][scode][1]+=1\n",
        "                else:\n",
        "                    frequencies[lexeme].update({scode:[sclass, 1]})\n",
        "\n",
        "            else:\n",
        "                frequencies.update({lexeme:{scode: [sclass, 1]}})\n",
        "\n",
        "    sclasses.remove(None)\n",
        "\n",
        "    from statistics import mean\n",
        "    from tqdm import tqdm\n",
        "    from collections import Counter\n",
        "    import os\n",
        "    path = \"./classifiers/\"\n",
        "    names = os.listdir(path)\n",
        "    strings=[\"lexeme\\tfrequency of lexeme\\tsclasses num\\tnum correct mlp\\tnum all mlp\\tmode\\tmodel\\tscore\\n\"]\n",
        "    scores=[]\n",
        "\n",
        "    for lex in tqdm(lexs):\n",
        "        abs_freq=0\n",
        "        target_vectors =[]\n",
        "        try:\n",
        "            sclasses = [frequencies[lex][scode][0] for scode in frequencies[lex]]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for vec in vecs:\n",
        "            if vec is not None:\n",
        "                test = vec[0]\n",
        "                training = vec[1]\n",
        "                lexeme = vec[2]\n",
        "                scode = vec[3]\n",
        "                sclass = vec[4]\n",
        "                \n",
        "                if lexeme == lex and sclass in sclasses:\n",
        "                    abs_freq+=1\n",
        "                    if mode != \"weighted\":\n",
        "                        target_vectors.append(test)\n",
        "                    elif mode == \"weighted\":\n",
        "                        target_vectors.append((test, lexeme, scode))\n",
        "        \n",
        "        if len(target_vectors)==0:\n",
        "            continue\n",
        "        \n",
        "        if mode == \"uniformed\":\n",
        "            vector = get_uniform_vector(target_vectors)\n",
        "        elif mode == \"weighted\":\n",
        "            vector = get_weighted_vector(target_vectors, frequencies)\n",
        "\n",
        "\n",
        "        clfs = pickle.load(open(\"./classifiers_all/general_classifiers.pkl\", \"rb\"))\n",
        "        clfs_test={0:[], 1:[]}\n",
        "\n",
        "        for sclass_candid in clfs[model]:\n",
        "            if sclass_candid in sclasses:\n",
        "                clfs_test[1].append(clfs[model][sclass_candid])\n",
        "            else:\n",
        "                clfs_test[0].append(clfs[model][sclass_candid])\n",
        "\n",
        "        answer_sheet=[] \n",
        "        if mode in [\"uniformed\", \"weighted\"]:\n",
        "            for mlp in clfs_test[0]: # the answer should be negative\n",
        "                binary = mlp.predict(vector)[0]\n",
        "                if binary == 0: # correct\n",
        "                    answer_sheet.append(1)\n",
        "                else: # incorrect\n",
        "                    answer_sheet.append(0)\n",
        "            \n",
        "            for mlp in clfs_test[1]: # the answer should be positive\n",
        "                binary = mlp.predict(vector)[0]\n",
        "                if binary == 1: # correct\n",
        "                    answer_sheet.append(1)\n",
        "                else: # incorrect\n",
        "                    answer_sheet.append(0)\n",
        "        else: # for general vectors\n",
        "            for vector in target_vectors:\n",
        "                for mlp in clfs_test[0]: # the answer should be negative\n",
        "                    binary = mlp.predict(vector)[0]\n",
        "                    if binary == 0: # correct\n",
        "                        answer_sheet.append(1)\n",
        "                    else: # incorrect\n",
        "                        answer_sheet.append(0)\n",
        "                \n",
        "                for mlp in clfs_test[1]: # the answer should be positive\n",
        "                    binary = mlp.predict(vector)[0]\n",
        "                    if binary == 1: # correct\n",
        "                        answer_sheet.append(1)\n",
        "                    else: # incorrect\n",
        "                        answer_sheet.append(0)\n",
        "\n",
        "        \n",
        "        corrects = Counter(answer_sheet)[1]\n",
        "        score = round((corrects /len(answer_sheet))*100, 4)\n",
        "        scores.append(score)\n",
        "        #abs_freq = sum([v[1] for k, v in frequencies[lex].items()])\n",
        "        strings.append(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(lex, abs_freq, len(sclasses), corrects, len(answer_sheet), mode, model, score))    \n",
        "        with open(\"./results/{}_{}_result.tsv\".format(model, mode), \"w\") as f:\n",
        "            f.write(\"\".join(strings))\n",
        "\n",
        "    strings.append(\"\\nAverage score: {}\\n\".format(round(mean(scores), 4)))\n",
        "    with open(\"./results/{}_{}_result.tsv\".format(model, mode), \"w\") as f:\n",
        "        f.write(\"\".join(strings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO8NZ8imIp5-"
      },
      "source": [
        "import pickle\n",
        "w2v_general_vecs = pickle.load(open(\"./vectors/concated_w2v_general_vectors.pkl\", \"rb\"))\n",
        "ft_general_vecs = pickle.load(open(\"./vectors/concated_ft_general_vectors.pkl\", \"rb\"))\n",
        "bert_general_vecs = pickle.load(open(\"./vectors/concated_bert_general_vectors.pkl\", \"rb\"))\n",
        "frequencies = pickle.load(open(\"./dictionary/matched_words/frequencies.pkl\", \"rb\"))\n",
        "sense_classes = pickle.load(open(\"./dictionary/sense_classes.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXXWa_DeIp5_",
        "outputId": "e225564b-029a-4c30-c846-6a579a05d6f7"
      },
      "source": [
        "experiment(model=\"w2v\", mode=\"weighted\", vecs=w2v_general_vecs, frequencies=frequencies)\n",
        "experiment(model=\"ft\", mode=\"weighted\", vecs=ft_general_vecs, frequencies=frequencies)\n",
        "experiment(model=\"bert\", mode=\"weighted\", vecs=bert_general_vecs, frequencies=frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6455/6455 [02:03<00:00, 52.10it/s]\n",
            "100%|██████████| 6455/6455 [02:02<00:00, 52.85it/s]\n",
            "100%|██████████| 6455/6455 [02:01<00:00, 53.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5McVMgtwIp5_",
        "outputId": "a914bdf6-62b2-4b27-e736-1f1bdf402fc8"
      },
      "source": [
        "experiment(model=\"w2v\", mode=\"uniformed\", vecs=w2v_general_vecs, frequencies=frequencies)\n",
        "experiment(model=\"ft\", mode=\"uniformed\", vecs=ft_general_vecs, frequencies=frequencies)\n",
        "experiment(model=\"bert\", mode=\"uniformed\", vecs=bert_general_vecs, frequencies=frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6455/6455 [01:57<00:00, 54.98it/s]\n",
            "100%|██████████| 6455/6455 [02:02<00:00, 52.67it/s]\n",
            "100%|██████████| 6455/6455 [02:07<00:00, 50.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyIXhT8YIp5_",
        "outputId": "d9ff26b7-d753-4e12-e3b0-2fc4d042fd52"
      },
      "source": [
        "experiment(model=\"w2v\", mode=\"general\", vecs=w2v_general_vecs, frequencies=frequencies)\n",
        "experiment(model=\"ft\", mode=\"general\", vecs=ft_general_vecs, frequencies=frequencies)\n",
        "experiment(model=\"bert\", mode=\"general\", vecs=bert_general_vecs, frequencies=frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6455/6455 [02:19<00:00, 46.39it/s]\n",
            "100%|██████████| 6455/6455 [02:22<00:00, 45.18it/s]\n",
            "100%|██████████| 6455/6455 [02:20<00:00, 45.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "03882971da944d9ab79587910701e9c2"
          ]
        },
        "id": "fdjp9ZnYIp6A",
        "outputId": "f4e54776-a3bf-4a28-d7a9-44296c0a3afb"
      },
      "source": [
        "def baseline_experiment(vecs):\n",
        "\n",
        "    # get all lexemes\n",
        "    lexs=set()\n",
        "    sclasses=set()\n",
        "    frequencies={}\n",
        "\n",
        "    for vec in vecs:\n",
        "        if vec is None:\n",
        "            continue\n",
        "        #test = vec[0]\n",
        "        #training = vec[1]\n",
        "        lexeme = vec[2]\n",
        "        scode = vec[3]\n",
        "        sclass = vec[4]\n",
        "        \n",
        "        lexs.add(lexeme)\n",
        "        sclasses.add(sclass)\n",
        "        if sclass is not None:\n",
        "            if lexeme in frequencies:\n",
        "                if scode in frequencies[lexeme]:\n",
        "                    frequencies[lexeme][scode][1]+=1\n",
        "                else:\n",
        "                    frequencies[lexeme].update({scode:[sclass, 1]})\n",
        "\n",
        "            else:\n",
        "                frequencies.update({lexeme:{scode: [sclass, 1]}})\n",
        "\n",
        "    sclasses.remove(None)\n",
        "\n",
        "    from statistics import mean\n",
        "    from collections import Counter\n",
        "    from tqdm.notebook import tqdm\n",
        "    from random import random\n",
        "    \n",
        "    strings=[\"lexeme\\tfrequency of lexeme\\tsclasses num\\tnum correct mlp\\tnum all mlp\\tscore\\n\"]\n",
        "    scores=[]\n",
        "\n",
        "    for lex in tqdm(lexs):\n",
        "        abs_freq=0\n",
        "        target_vectors =[]\n",
        "        try:\n",
        "            sclasses = [frequencies[lex][scode][0] for scode in frequencies[lex]]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for vec in vecs:\n",
        "            if vec is not None:\n",
        "                lexeme = vec[2]\n",
        "                sclass = vec[4]\n",
        "                \n",
        "                if lexeme == lex and sclass in sclasses:\n",
        "                    abs_freq+=1\n",
        "\n",
        "        clfs = pickle.load(open(\"./classifiers_all/general_classifiers.pkl\", \"rb\"))\n",
        "        clfs_test={0:[], 1:[]}\n",
        "\n",
        "        for sclass_candid in clfs[\"w2v\"]:\n",
        "            if sclass_candid in sclasses:\n",
        "                clfs_test[1].append(clfs[\"w2v\"][sclass_candid])\n",
        "            else:\n",
        "                clfs_test[0].append(clfs[\"w2v\"][sclass_candid])\n",
        "\n",
        "        \n",
        "        answer_sheet=[] \n",
        "        \n",
        "        for mlp in clfs_test[0]: # the answer should be negative\n",
        "            binary = int(random() *2)\n",
        "            if binary == 0: # correct\n",
        "                answer_sheet.append(1)\n",
        "            else: # incorrect\n",
        "                answer_sheet.append(0)\n",
        "        \n",
        "        for mlp in clfs_test[1]: # the answer should be positive\n",
        "            binary = int(random() *2)\n",
        "            if binary == 1: # correct\n",
        "                answer_sheet.append(1)\n",
        "            else: # incorrect\n",
        "                answer_sheet.append(0)\n",
        "    \n",
        "        \n",
        "        corrects = Counter(answer_sheet)[1]\n",
        "        score = round((corrects /len(answer_sheet))*100, 4)\n",
        "        scores.append(score)\n",
        "        #abs_freq = sum([v[1] for k, v in frequencies[lex].items()])\n",
        "        strings.append(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(lex, abs_freq, len(sclasses), corrects, len(answer_sheet), score))    \n",
        "        with open(\"./results/sense_detection_{}_result.tsv\".format(\"baseline\"), \"w\") as f:\n",
        "            f.write(\"\".join(strings))\n",
        "\n",
        "    strings.append(\"\\nAverage score: {}\\n\".format(round(mean(scores), 4)))\n",
        "    with open(\"./results/sense_detection_{}_result.tsv\".format(\"baseline\"), \"w\") as f:\n",
        "        f.write(\"\".join(strings))\n",
        "\n",
        "import pickle\n",
        "w2v_general_vecs = pickle.load(open(\"./vectors/concated_w2v_general_vectors.pkl\", \"rb\"))\n",
        "baseline_experiment(w2v_general_vecs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6455.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03882971da944d9ab79587910701e9c2"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}